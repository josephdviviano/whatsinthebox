# Feature Engineering

Four types of feature spaces were engineered including TF-IDF (weighted score and matrix), N-grams (char and word level), typed-dependencies, and sentiment scores. Different text-corpus pre-processing steps were required for each of the feature spaces, which will be described in detail below.

- [clean_tweets.py](https://github.com/tpawelski/hate-speech-detection/edit/master/feature%20engineering%20scripts/clean_tweets.py): performs general pre-processing steps on the initial dataset including punctuation removal and converting all characters to lower-case 

- [stanford_nlp.py](https://github.com/tpawelski/hate-speech-detection/blob/master/feature%20engineering%20scripts/stanford_nlp.py): word structure within a tweet is preserved using the typed dependency tool of the Stanford Parser, which identifies syntactic relationships between words in a sentence. This script runs the Stanford Parser on the dataset and returns the typed dependiencies for each tweet as a dictionary (each key represents tweet index, each values is a list of typed dependencies for that tweet). The dictionary is then output as a .json file, [dependency_dict.json](https://github.com/tpawelski/hate-speech-detection/blob/master/feature%20engineering%20scripts/dependency_dict.json)

- [dependency_features.py](https://github.com/tpawelski/hate-speech-detection/blob/master/feature%20engineering%20scripts/dependency_features.py): reads in .json file containing typed dependencies and creates dependency based features. The relationship within each tweet identified by the Stanford Parser resulted in 41 features, with each storing the counts of each different typed dependency

- [ngram_features.py](https://github.com/tpawelski/hate-speech-detection/blob/master/feature%20engineering%20scripts/ngram_features.py): uses a Count Vectorizer to create bi-gram features at both word and character levels. The Count Vectorizer is also used to create TF-IDF matrix. For TF-IDF, word bi-grams, and character bi-grams, the stop words were removed and the remaining tokens were stemmed. Subsequently, for word bi-grams and TF-IDF, symbols were removed so that tokens such as “#Selfie,” for example, would not be counted differently than “Selfie.” Numeric digits however, were not removed from the text so that the emojis in Unicode form were preserved as tokens. For character bi-grams, on the other hand, symbols were kept in the text to capture the typographic symbols that are commonly used to censor profanities such as “b*tch” or “a$$hole”. Numeric digits were re- moved when constructing character bi-grams as Unicode representation does not provide value at the character level.

- [sentiment_scores.py](https://github.com/tpawelski/hate-speech-detection/blob/master/feature%20engineering%20scripts/sentiment_scores.py): the number of vocabularies that appears in the hate base , negative and positive dictionaries are counted for each tweet. As some tweets with negative words could also express positive meaning (e.g. f*cking awesome), positive lexicons are added for a more holistic sentiment analysis. Moreover, sentiment scores, namely the normalization of positive and negative term frequency, were calculated by dividing the length of tweet, based on the assumption that given the same frequency of negative/positive lexicons, a shorter tweet shows stronger sentiment than a longer tweet.

- [tf-idf.py](https://github.com/tpawelski/hate-speech-detection/blob/master/feature%20engineering%20scripts/tf-idf.py): TF-IDF scores, used as the only feature set to be included in the baseline model, are created based on the term frequency and inverse document frequency. For each tweet, TF-IDF values are calculated for each word and assigned a weight of 1 if the word appears in the hate base dictionary, 0 otherwise. The weightings are simply set at 1 and 0 since we're only concerned about words appearing in the dictionary. All TF-IDFs are then added together after multiplying their corresponding weight as TF-IDF score for each tweet
